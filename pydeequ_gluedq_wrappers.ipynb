{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQDL Wrappers for PyDeequ\n",
    "\n",
    "This notebook expands upon the [basic_example](basic_example.ipynb) to define a Class having a set of 'quality' methods similar to the boto3 Glue.Class.   This provides a custom implementation of DLDQ-like functionality with alternate data stores in DynamoDb and S3, while supporting easy future migration to Glue ETL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# cold start (after notebook restart)\n",
    "pip install pydeequ\n",
    "pip install 'awswrangler[redshift]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cold start for SageMaker on WC2H -- unzip dependencies from local file to Ivy cache, since Maven is blocked\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "S_rootdir = os.getcwd()\n",
    "\n",
    "with zipfile.ZipFile( f\"{S_rootdir}/common/ivy2cache.zip\" ) as z:\n",
    "    z.extractall( f\"{os.environ['HOME']}/.ivy2\" )\n",
    "\n",
    "with zipfile.ZipFile( f\"{S_rootdir}/common/ivy2cache_33.zip\" ) as z:\n",
    "    z.extractall( f\"{os.environ['HOME']}/.ivy2\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c3cb4f5a-c5a3-4890-8be3-b88592caf142;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;2.0.3-spark-3.3 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      ":: resolution report :: resolve 357ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;2.0.3-spark-3.3 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.1 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   2   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c3cb4f5a-c5a3-4890-8be3-b88592caf142\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/9ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/26 16:37:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/26 16:37:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/09/26 16:37:30 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/09/26 16:37:30 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/09/26 16:37:30 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/09/26 16:37:30 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "# SageMaker initialization\n",
    "\n",
    "import os \n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-gov-west-1'\n",
    "#os.environ[\"SPARK_VERSION\"] = '3.0'\n",
    "os.environ[\"SPARK_VERSION\"] = '3.3'\n",
    "\n",
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "\n",
    "import sagemaker_pyspark\n",
    "import pydeequ\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "from pyspark import SparkConf\n",
    "conf = (SparkConf()\n",
    "        .set('fs.s3a.endpoint', 's3-us-gov-west-1.amazonaws.com')\n",
    "        .set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
    "        .set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "        .set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
    "        .set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")        \n",
    "       )\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .config( conf=conf )\n",
    "    .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert( 0, './python') \n",
    "\n",
    "#  Common code module for SIMPLE Data Quality\n",
    "import dq_common_2309 as dq_comm\n",
    "dq = dq_comm.SimpleDQ( spark )\n",
    "\n",
    "# Common code module for SIMPLE File Processing\n",
    "import batch_simple_2309 as bat\n",
    "\n",
    "import importlib\n",
    "importlib.reload(dq_comm)\n",
    "importlib.reload(bat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Scripts\n",
    "The following cells demonstrate Data Quality functionality implemented in the common code libraries.  This functionality has been copied into [Glue ETL job scripts](./glue) for batch execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generate Rule Recommendations\n",
    "\n",
    "Rule recommendations utilize [Deequ *constraint suggestion*](https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/examples/constraint_suggestion_example.md) functionality to first profile all columns in the data set, and then apply heuristic rules to define a set of suggested constraints.\n",
    "\n",
    "This library adds a [DQDL-like Rule](https://docs.aws.amazon.com/glue/latest/dg/dqdl.html) to each suggested constraint, and stores the suggested constraints in **dqsuggestion_runs** DynamoDB table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate Rule Recommendations\n",
    "s3_url = \"s3a://{env-prefix}-datalake/PARQUET/tstg_creditor_agency/Unload.D230831/\" \n",
    "\n",
    "arg = {\n",
    "    \"DataSource\" : {\n",
    "        \"S3Url\" : s3_url\n",
    "    }\n",
    "}\n",
    "\n",
    "run_id = dq.start_data_quality_rule_recommendation_run( **arg )\n",
    "run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1a. Create Data Quality Ruleset from Recommendations\n",
    "Suggested Constraints provide a lot of detailed information about every column in the dataset. This library supports storing DQDL-like rules in a more concise format similar to Glue Data Quality API.  Rulesets stored in **dqrulesets** DynamoDB table can be edited to use only desired constraints, or to modify conditions applied (Hint -- use DynamoDB Form mode to edit the Ruleset string).  Alternatively, Rulesets can be defined from scratch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1a. Convert Recommendations to a Ruleset\n",
    "\n",
    "print(run_id) # from Step 1\n",
    "\n",
    "tablename = dq.get_tablename_from_datasource( arg['DataSource'] ) # from Step 1\n",
    "\n",
    "rec = dq.get_data_quality_rule_recommendation_run( **run_id )\n",
    "rec\n",
    "rules_list = []\n",
    "for item in rec['ConstraintSuggestions']:\n",
    "    rules_list.append( item['dqdl_rule'] )\n",
    "    \n",
    "#print(rules_list)\n",
    "\n",
    "ruleset = {\n",
    " \"Name\": f\"{tablename}_notebook\",\n",
    " \"ClientToken\": \"string\",\n",
    " \"Description\": \"ruleset from pydeequ.suggestions\",\n",
    " \"Ruleset\": \", \\n\".join(rules_list) , # stored as string\n",
    " \"Tags\": {\n",
    "  \"SuggestionRunId\": rec[\"RunId\"]\n",
    " },\n",
    " \"DataSource\" : rec[\"DataSource\"],    # custom  \n",
    " \"TargetTable\": {\n",
    "  \"CatalogId\": \"AwsDataCatalog\",\n",
    "  \"DatabaseName\": \"dtl-prd-smpl0-g2\", # ToDo from variable\n",
    "  \"TableName\": tablename\n",
    " }\n",
    "}\n",
    "\n",
    "dq.update_data_quality_ruleset(**ruleset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run Ruleset to Evaluate Data Quality\n",
    "Next, the Ruleset can be evaluated against particular data sets to verify compliance with each of the constraints.   Ruleset Evaluation Runs are logged in **dqruleset-eval-runs** DynamoDB table, which associates the evaluated Datasource with Rulesets applied and the Results of the evaluation.   Multiple Rulesets can be applied against a DataSource in the same Evaluation Run, to generate multiple Results stored in **dqresults** table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2. Run Ruleset to Evaluate Data Quality\n",
    "\n",
    "ruleset_name = f\"{tablename}_notebook\"  # from Step 1a\n",
    "\n",
    "ruleset = dq.get_data_quality_ruleset( Name = ruleset_name )\n",
    "#print(ruleset)\n",
    "\n",
    "# note the option to evaluate multiple Rulesets by name in same run\n",
    "ruleset_runspec = {\n",
    "    'DataSource': ruleset['DataSource'],\n",
    "    'RulesetNames' : [\n",
    "        ruleset['Name'],\n",
    "    ]\n",
    "}\n",
    "\n",
    "eval_run_id = dq.start_data_quality_ruleset_evaluation_run( **ruleset_runspec)\n",
    "\n",
    "dq_eval_run = dq.get_data_quality_ruleset_evaluation_run( **eval_run_id )\n",
    "\n",
    "# note that a ResultId will be generated for each Ruleset\n",
    "for result_id in dq_eval_run['ResultIds']:\n",
    "    dq_result = dq.get_data_quality_result( ResultId = result_id )\n",
    "\n",
    "#print(json.dumps(dq_result, default=str, indent=2))\n",
    "print( f\"Score: {dq_result['Score']}\")\n",
    "#print( f\"Tally: {dq_result['Tally']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test\n",
    "dq = dq_comm.SimpleDQ(spark)\n",
    "\n",
    "ruleset_runspec = {'DataSource': {'S3Url': f's3://{env-prefix}-datalake/PARQUET/tstg_creditor_agency/Unload.D230831/'},\n",
    " 'RulesetNames': ['tstg_creditor_agency_generated']}\n",
    "\n",
    "eval_run_id = dq.start_data_quality_ruleset_evaluation_run( **ruleset_runspec)\n",
    "\n",
    "print( eval_run_id )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run Data Profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3. Run Data Profiling\n",
    "s3_url = \"s3a://{env-prefix}-datalake/PARQUET/tstg_creditor_agency/Unload.D230831/\" # OK\n",
    "\n",
    "arg = {\n",
    "    \"DataSource\" : {\n",
    "        \"S3Url\" : s3_url\n",
    "    }\n",
    "}\n",
    "\n",
    "run_id = dq.start_data_quality_profile_run( **arg )\n",
    "run_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Export Metrics from Dynamodb to S3\n",
    "This section demonstrates how to export JSON files to 'flat' parquet files that are cataloged as Glue tables.\n",
    "\n",
    "Work in progress!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sample Batch Item copied from DynamoDB Batch table\n",
    "batch_item = {\n",
    " \"BatchId\": \"DQ.TDSX.BASELINE.D230902\",\n",
    " \"ObjectId\": \"tdsx_dream_table\",\n",
    " \"Expiry\": \"1697981640.105374\",\n",
    " \"Status\": \"COMPLETED\",\n",
    " \"Step-DQ_Profile_DataSources\": {\n",
    "  \"DataSource\": {\n",
    "   \"S3Url\": \f"s3://{env-prefix}-datalake/PARQUET/tdsx_dream_table/D230902.Extract/\"\n",
    "  },\n",
    "  \"GlueJobId\": \"jr_350d58970ba362596e0a295cb57498811d1fdf58f2b92dd8fac8d435eddd320c\",\n",
    "  \"ProfileRunId\": \"dqprofile-2023-09-20T13:08:21.002582Z-dtl-prd-SMPL0-DQ_Profile_DataSources\",\n",
    "  \"SfnExecName\": \"230920-130300-DQ.TDSX.BASELINE.D230902\",\n",
    "  \"Status\": \"COMPLETED\",\n",
    "  \"StatusMsg\": \"dqprofile-2023-09-20T13:08:21.002582Z-dtl-prd-SMPL0-DQ_Profile_DataSources\",\n",
    "  \"TimeStamp\": \"2023-09-20 13:15:17.400448\"\n",
    " },\n",
    " \"Step-DQ_Recommend_Rulesets\": {\n",
    "  \"GlueJobId\": \"jr_a1906050d074f457f904cefb3025cca0b4e33cb64c31e6cb317914e498aa21cc\",\n",
    "  \"RulesetName\": \"tdsx_dream_table_generated\",\n",
    "  \"SfnExecName\": \"230920-130300-DQ.TDSX.BASELINE.D230902\",\n",
    "  \"Status\": \"COMPLETED\",\n",
    "  \"StatusMsg\": \"tdsx_dream_table_generated\",\n",
    "  \"SuggestionRunId\": \"dqrecrun-2023-09-20T13:22:28.323116Z-dtl-prd-SMPL0-DQ_Suggestions\",\n",
    "  \"TimeStamp\": \"2023-09-20 13:33:06.905505\"\n",
    " },\n",
    " \"Step-DQ_Suggestions\": {\n",
    "  \"DataSource\": {\n",
    "   \"S3Url\": \f"s3://{env-prefix}-datalake/PARQUET/tdsx_dream_table/D230902.Extract/\"\n",
    "  },\n",
    "  \"GlueJobId\": \"jr_6b06fede1f1df257e67f5e4d1ecf5db174a5faaa10283930d93d94c396074b26\",\n",
    "  \"SfnExecName\": \"230920-130300-DQ.TDSX.BASELINE.D230902\",\n",
    "  \"Status\": \"COMPLETED\",\n",
    "  \"StatusMsg\": \"dqrecrun-2023-09-20T13:22:28.323116Z-dtl-prd-SMPL0-DQ_Suggestions\",\n",
    "  \"SuggestionRunId\": \"dqrecrun-2023-09-20T13:22:28.323116Z-dtl-prd-SMPL0-DQ_Suggestions\",\n",
    "  \"TimeStamp\": \"2023-09-20 13:29:10.915159\"\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo -- local functions will be moved to dq_common module\n",
    "\n",
    "def catalog_exports_in_glue( glue_dbname, glue_table_name, folders, partion_cnt=1 ):\n",
    "    '''\n",
    "    glue_dbname = 'dtl-prd-smpl0-dq' # ToDo - get from Batch Rec ProcessParms\n",
    "    glue_table_name = \"dq_results\"\n",
    "    glue_table_path = \f"s3://{env-prefix}-datalake/DQRESULT\"\n",
    "    partition_cnt=2\n",
    "    pq_meta = wr.s3.read_parquet_metadata(path=\f"s3://{env-prefix}-datalake/DQRESULT/tdsx_analytics_dispute_dashboard/D230925.EVALRULES/\")\n",
    "    glue_coltypes = pq_meta.columns_types #.values()\n",
    "\n",
    "    bat.set_glue_db_and_table( glue_dbname, f\"{glue_table_name}_latest\", glue_table_path, glue_coltypes, partition_cnt=2 )\n",
    "    '''\n",
    "    glue_tables = wr.catalog.tables(database=glue_dbname, limit=1000)\n",
    "    if glue_table_name not in glue_tables['Table'].tolist():        \n",
    "        print( f\"Please Create Table {glue_table_name} in Database {glue_dbname}\")\n",
    "        return\n",
    "\n",
    "    for folder in folders:\n",
    "\n",
    "        bat.set_glue_table_partitions( glue_dbname, f\"{glue_table_name}_latest\", partition_s3url, partition_cnt, mode='replace')\n",
    "        bat.set_glue_table_partitions( glue_dbname, f\"{glue_table_name}_series\", partition_s3url, partition_cnt, mode='append')\n",
    "\n",
    "        #break\n",
    "\n",
    "    return\n",
    "\n",
    "def export_dynamodb_to_parquet( batch_item ):\n",
    "\n",
    "    tablename = batch_item['ObjectId']\n",
    "    file_dt = batch_item['BatchId'].split('.')[-1]\n",
    "    partition = f\"{batch_item['BatchId'].split('.')[-1]}.{batch_item['BatchId'].split('.')[-2]}\"\n",
    "\n",
    "    # PROFILES\n",
    "    profile = dq.get_data_quality_profile( RunId = batch_item['Step-DQ_Profile_DataSources']['ProfileRunId'] )\n",
    "    s3_url = profile['DataSource']['S3Url']\n",
    "    bucket = s3_url[5:s3_url.find('/',5)]\n",
    "    \n",
    "    # build flattened dataframe from json\n",
    "    df_profile = pd.DataFrame.from_dict(profile['ColumnProfiles'], orient='columns')\n",
    "    df_profile = df_profile[['column_name', 'profile_type', 'isDataTypeInferred', 'dataType', 'completeness','kll', 'sum', 'mean', 'maximum', 'stdDev', 'minimum']]\n",
    "    df_profile['table_name'] = tablename\n",
    "    df_profile['profile_run_id'] = batch_item['Step-DQ_Profile_DataSources']['ProfileRunId']\n",
    "\n",
    "    # write profile to S3 & Glue\n",
    "    s3_urlout = f\"s3://{bucket}/DQPROFILE/{tablename}/{partition}/part-0.parquet\"\n",
    "    #store_export_in_S3_and_Glue( df_profile, s3_urlout )\n",
    "    wr.s3.to_parquet (\n",
    "        df = df,\n",
    "        path = s3_urlout\n",
    "    )\n",
    "    \n",
    "    # SUGGESTIONS\n",
    "    suggestions = dq.get_data_quality_suggestions( RunId = batch_item['Step-DQ_Suggestions']['SuggestionRunId'] )\n",
    "\n",
    "    # build flattened dataframe from json\n",
    "    df_suggestion = pd.DataFrame.from_dict(suggestions['ConstraintSuggestions'], orient='columns')\n",
    "    df_suggestion = df_suggestion[['column_name', 'dqdl_rule', 'description', 'code_for_constraint', 'constraint_name', 'rule_description', 'suggesting_rule',  'current_value' ]]\n",
    "    df_suggestion['table_name'] = tablename\n",
    "    df_suggestion['suggestion_run_id'] = batch_item['Step-DQ_Suggestions']['SuggestionRunId']\n",
    "\n",
    "    # write profile to S3 & Glue\n",
    "    s3_urlout = f\"s3://{bucket}/DQSUGGEST/{tablename}/{partition}/part-0.parquet\"\n",
    "    store_export_in_S3_and_Glue( df_suggestion, s3_urlout )\n",
    "    wr.s3.to_parquet (\n",
    "        df = df,\n",
    "        path = s3_urlout\n",
    "    )\n",
    "    return s3_urlout\n",
    "\n",
    "    '''   \n",
    "    # RESULTS are exported in Glue ETL job DQ_Evaluate_Rulesets\n",
    "       \n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = '{env-prefix}-datalake'\n",
    "pattern = \".\"\n",
    "s3_urlout = export_dynamodb_to_parquet( batch_item )\n",
    "\n",
    "tables = [\n",
    "    \"DQPROFILE\" , \"DQSUGGEST\", \"DQRESULT\"\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    folders = bat.get_namelist_by_S3pattern( s3_bucket, pattern, ExpandFolders = False , Folder= table)\n",
    "    catalog_exports_in_glue( glue_dbname, glue_table_name, folders, partion_cnt=2 )\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
